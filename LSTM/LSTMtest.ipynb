{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf55625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T12:51:10.658136400Z",
     "start_time": "2024-03-06T12:50:51.097948600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3.8\\envs\\battery\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# hyper\n",
    "features = ['SOH','voltage_measured', 'current_measured',\n",
    "            'temperature_measured', 'time']\n",
    "batch_size = 1\n",
    "input_size = len(features)\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "seq_len = 30   # 预测序列长度\n",
    "epoch = 500\n",
    "learning_rate = 0.001  # 自适应学习率？\n",
    "\n",
    "save_path = 'seq50_.pth'\n",
    "train_directory = '../datasets/train/'\n",
    "verify_directory = '../datasets/verify/'\n",
    "test_directory = '../datasets/alldataset/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T14:59:51.006468300Z",
     "start_time": "2024-03-06T14:59:50.964676800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Data preprocess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load Data from mat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 53\u001B[0m\n\u001B[0;32m     51\u001B[0m test_battery_id2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB0055\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     52\u001B[0m test_battery_id3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB0056\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 53\u001B[0m dataset0, capacity0 \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_directory\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtest_battery_id0\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.mat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m dataset1, capacity1 \u001B[38;5;241m=\u001B[39m load_data(test_directory\u001B[38;5;241m+\u001B[39m test_battery_id1 \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mat\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     55\u001B[0m dataset2, capacity2 \u001B[38;5;241m=\u001B[39m load_data(test_directory\u001B[38;5;241m+\u001B[39m test_battery_id2 \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mat\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[52], line 21\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(battery_path)\u001B[0m\n\u001B[0;32m     15\u001B[0m date_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime(\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]),\n\u001B[0;32m     16\u001B[0m                               \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m1\u001B[39m]),\n\u001B[0;32m     17\u001B[0m                               \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m2\u001B[39m]),\n\u001B[0;32m     18\u001B[0m                               \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m3\u001B[39m]),\n\u001B[0;32m     19\u001B[0m                               \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m4\u001B[39m])) \u001B[38;5;241m+\u001B[39m datetime\u001B[38;5;241m.\u001B[39mtimedelta(seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m5\u001B[39m]))\n\u001B[0;32m     20\u001B[0m data \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 21\u001B[0m capacity \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCapacity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVoltage_measured\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])):\n\u001B[0;32m     23\u001B[0m     voltage_measured \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVoltage_measured\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][j]\n",
      "\u001B[1;31mIndexError\u001B[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "def load_data(battery_path):\n",
    "    # mat = loadmat('../datasets/BatteryDataset/' + battery_id + '.mat') # MAT PATH ESSENTIAL origin\n",
    "    battery_id = battery_path[-9:-4]\n",
    "    mat = loadmat(battery_path) # MAT PATH ESSENTIAL\n",
    "\n",
    "    counter = 0\n",
    "    dataset = []\n",
    "    capacity_data = []\n",
    "    # print('Total data in dataset: ', len(mat[battery][0, 0]['cycle'][0])) #total cycle\n",
    "\n",
    "    for i in range(len(mat[battery_id][0, 0]['cycle'][0])):\n",
    "        row = mat[battery_id][0, 0]['cycle'][0, i]\n",
    "        if row['type'][0] == 'discharge':\n",
    "            ambient_temperature = row['ambient_temperature'][0][0]\n",
    "            date_time = datetime.datetime(int(row['time'][0][0]),\n",
    "                                          int(row['time'][0][1]),\n",
    "                                          int(row['time'][0][2]),\n",
    "                                          int(row['time'][0][3]),\n",
    "                                          int(row['time'][0][4])) + datetime.timedelta(seconds=int(row['time'][0][5]))\n",
    "            data = row['data']\n",
    "            capacity = data[0][0]['Capacity'][0][0]\n",
    "            for j in range(len(data[0][0]['Voltage_measured'][0])):\n",
    "                voltage_measured = data[0][0]['Voltage_measured'][0][j]\n",
    "                current_measured = data[0][0]['Current_measured'][0][j]\n",
    "                temperature_measured = data[0][0]['Temperature_measured'][0][j]\n",
    "                current_load = data[0][0]['Current_load'][0][j]\n",
    "                voltage_load = data[0][0]['Voltage_load'][0][j]\n",
    "                time = data[0][0]['Time'][0][j]\n",
    "                dataset.append([counter + 1, ambient_temperature, date_time, capacity,\n",
    "                                voltage_measured, current_measured,temperature_measured,\n",
    "                                current_load, voltage_load, time])\n",
    "\n",
    "            capacity_data.append([counter + 1, ambient_temperature, date_time, capacity,\n",
    "                                  voltage_measured, current_measured,temperature_measured,\n",
    "                                  current_load, voltage_load, time])\n",
    "            counter = counter + 1\n",
    "    # print(dataset[0])\n",
    "\n",
    "    return [pd.DataFrame(data=dataset,\n",
    "                         columns=['cycle', 'ambient_temperature', 'datetime','capacity',\n",
    "                                  'voltage_measured','current_measured', 'temperature_measured',\n",
    "                                  'current_load', 'voltage_load', 'time']),\n",
    "            pd.DataFrame(data=capacity_data,\n",
    "                         columns=['cycle', 'ambient_temperature', 'datetime','capacity',\n",
    "                                  'voltage_measured','current_measured', 'temperature_measured',\n",
    "                                  'current_load', 'voltage_load', 'time'])]\n",
    "\n",
    "\n",
    "# test_battery_id0 = 'B0050'\n",
    "# test_battery_id1 = 'B0052'\n",
    "# test_battery_id2 = 'B0055'\n",
    "# test_battery_id3 = 'B0056'\n",
    "# dataset0, capacity0 = load_data(test_directory+ test_battery_id0 + '.mat')\n",
    "# dataset1, capacity1 = load_data(test_directory+ test_battery_id1 + '.mat')\n",
    "# dataset2, capacity2 = load_data(test_directory+ test_battery_id2 + '.mat')\n",
    "# dataset3, capacity3 = load_data(test_directory+ test_battery_id3 + '.mat')\n",
    "# # capacity.to_csv('../datasets/BatteryCSV/{}.csv'.format(test_battery_id), index=False)\n",
    "# # pd.set_option('display.max_columns', 10)\n",
    "# # print(capacity.head(5))\n",
    "# # dataset.describe()\n",
    "#\n",
    "# # 创建一个包含两个子图的 Figure 对象\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "# sns.set_style(\"darkgrid\") # 黑色的背景，带有白色网格线\n",
    "# # 在第一个子图中绘制 Sin(x)\n",
    "# axs[0, 0].plot(capacity0['cycle'], capacity0['capacity'])\n",
    "# axs[0, 0].set_title('Discharge {}'.format(test_battery_id0))\n",
    "# axs[0, 0].set_xlabel('cycle')\n",
    "# axs[0, 0].set_xlabel('Capacity')\n",
    "# # 在第二个子图中绘制 Cos(x)\n",
    "# axs[0, 1].plot(capacity1['cycle'], capacity1['capacity'])\n",
    "# axs[0, 1].set_title('Discharge {}'.format(test_battery_id1))\n",
    "# axs[0, 1].set_xlabel('cycle')\n",
    "# axs[0, 1].set_xlabel('Capacity')\n",
    "#\n",
    "# # 在第三个子图中绘制 Cos(x)\n",
    "# axs[1, 0].plot(capacity2['cycle'], capacity2['capacity'])\n",
    "# axs[1, 0].set_title('Discharge {}'.format(test_battery_id2))\n",
    "# axs[1, 0].set_xlabel('cycle')\n",
    "# axs[1, 0].set_xlabel('Capacity')\n",
    "#\n",
    "# # 在第四个子图中绘制 Cos(x)\n",
    "# axs[1, 1].plot(capacity3['cycle'], capacity3['capacity'])\n",
    "# axs[1, 1].set_title('Discharge {}'.format(test_battery_id3))\n",
    "# axs[1, 1].set_xlabel('cycle')\n",
    "# axs[1, 1].set_xlabel('Capacity')\n",
    "#\n",
    "# # 调整布局\n",
    "# plt.tight_layout()\n",
    "# # 显示图像\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T14:52:51.222394500Z",
     "start_time": "2024-03-06T14:52:51.007394400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 normlize data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def norm_data(battery_id):\n",
    "    dataset, capacity = load_data(battery_id)\n",
    "    if capacity['capacity'][0] > capacity['capacity'][1] :\n",
    "        C = capacity['capacity'][0]\n",
    "    else :\n",
    "        C = capacity['capacity'][1]\n",
    "    soh = []\n",
    "    for i in range(len(capacity)):\n",
    "        soh.append([capacity['capacity'][i] / C])\n",
    "    soh = pd.DataFrame(data=soh, columns=['SOH'])\n",
    "\n",
    "    # features for training\n",
    "    attribs=['capacity', 'voltage_measured', 'current_measured',\n",
    "             'temperature_measured', 'current_load', 'voltage_load', 'time']\n",
    "    train_dataset = capacity[attribs]\n",
    "    sc = MinMaxScaler(feature_range=(0,1)) # = (num-min)/(max-min)\n",
    "    train_dataset = sc.fit_transform(train_dataset) # issue：not based on Rated\n",
    "    # print(train_dataset.shape)\n",
    "    # print(soh.shape)\n",
    "    attribs_scaled = pd.DataFrame(data=train_dataset,columns=attribs)\n",
    "    return  pd.concat([capacity['cycle'], attribs_scaled, soh], axis=1)\n",
    "\n",
    "def data_loader(battery_id, seq_len, features):\n",
    "    dataset = norm_data(battery_id)\n",
    "    input_size = len(features)\n",
    "    data_set_train=dataset[features].values\n",
    "    x_train=[]\n",
    "    label=[]\n",
    "    batch = len(data_set_train)\n",
    "    #take the last 10t to predict 10t+1\n",
    "    for i in range (seq_len,batch):\n",
    "        x_train.append(data_set_train[i-seq_len:i,:])\n",
    "        label.append(data_set_train[i,0])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_train = np.reshape(x_train,(batch-seq_len,seq_len,input_size)) #(batch,seq_len,input_size)\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "    label = torch.tensor(label, dtype=torch.float32).view(-1, 1) # (batch,seq_len)取最后一个输出\n",
    "    return x_train, label\n",
    "\n",
    "# # storge\n",
    "# features = ['SOH','voltage_measured', 'current_measured',\n",
    "#             'temperature_measured', 'current_load', 'voltage_load', 'time']\n",
    "# train_data,train_data_real = data_loader('B0005', 10 , features)\n",
    "# train_data,train_data_real\n",
    "# dataset_scaled.to_csv('../datasets/BatteryCSV/B0005norm.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T14:59:59.689765300Z",
     "start_time": "2024-03-06T14:59:59.642762400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, root_dir, seq_len, features,  transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.seq_len  = seq_len\n",
    "        self.features = features\n",
    "        self.transform = transform\n",
    "        self.file_list = sorted(os.listdir(root_dir))  # Assumes file names determine order\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        battery_id = self.file_list[idx]\n",
    "        battery_path = os.path.join(self.root_dir, battery_id)\n",
    "        dataset, label = data_loader(battery_path, self.seq_len, self.features)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     dataset = self.transform(dataset)\n",
    "        #     label = self.transform(label)\n",
    "\n",
    "        return dataset, label\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# custom_dataset = LoadDataset(train_directory, seq_len=seq_len, features=features)\n",
    "#\n",
    "# # Create DataLoader\n",
    "# batch_size = 1\n",
    "# dataloader = DataLoader(custom_dataset, batch_size=batch_size)\n",
    "#\n",
    "# for train_data, train_data_real in dataloader:\n",
    "#     # train_data=torch.squeeze(train_data)\n",
    "#     print(train_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T12:51:30.530786300Z",
     "start_time": "2024-03-06T12:51:30.511784500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. training\n",
    "class torch.nn.LSTM(*args, **kwargs)\n",
    "参数有：\n",
    "    input_size：x的特征维度\n",
    "    hidden_size：隐藏层的特征维度\n",
    "    num_layers：lstm隐层的层数，默认为1\n",
    "    bias：False则bihbih=0和bhhbhh=0. 默认为True\n",
    "    batch_first：True则输入输出的数据格式为 (batch, seq_len, feature)\n",
    "    dropout：除最后一层，每一层的输出都进行dropout，默认为: 0\n",
    "    bidirectional：True则为双向lstm默认为False\n",
    "\n",
    "LSTM input(seq_len, batch, input_size)\n",
    "参数有：\n",
    "    batch：每次喂给网络的数据条数，在NLP中就是一次喂给网络多少个句子 !!!\n",
    "    seq_len：序列长度，在NLP中就是句子长度，一般都会用pad_sequence补齐长度!!!\n",
    "    input_size：特征维度，和前面定义网络结构的input_size一致。\n",
    "\n",
    "output,(ht, ct) = net(input)\n",
    "    output: 最后一个状态的隐藏层的神经元输出 size = （seq_Len, batch, num_directions * hidden_size)\n",
    "    ht：最后一个状态的隐含层的状态值\n",
    "    ct：最后一个状态的隐含层的遗忘门值\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# net structure\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out, _ = self.lstm1(x) # shape（seq_Len, batch, num_directions * hidden_size)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out, _ = self.lstm4(out)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True) # 填充序列\n",
    "\n",
    "        out = self.fc(out[:, -1, :])  # 使用最后一个时间步的输出的输入（seq_Len, batch, num_directions * hidden_size)\n",
    "        # print(out.shape)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T12:51:34.879019600Z",
     "start_time": "2024-03-06T12:51:34.847020100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n",
      "-------epoch  1 -------\n",
      "train set Loss: 0.22439415752887726\n",
      "val set Loss: 0\n",
      "-------epoch  2 -------\n",
      "train set Loss: 0.012319139204919338\n",
      "val set Loss: 0\n",
      "-------epoch  3 -------\n",
      "train set Loss: 0.089961938560009\n",
      "val set Loss: 0\n",
      "-------epoch  4 -------\n",
      "train set Loss: 0.017450444400310516\n",
      "val set Loss: 0\n",
      "-------epoch  5 -------\n",
      "train set Loss: 0.05930446833372116\n",
      "val set Loss: 0\n",
      "-------epoch  6 -------\n",
      "train set Loss: 0.07030156254768372\n",
      "val set Loss: 0\n",
      "-------epoch  7 -------\n",
      "train set Loss: 0.07328835874795914\n",
      "val set Loss: 0\n",
      "-------epoch  8 -------\n",
      "train set Loss: 0.04389675334095955\n",
      "val set Loss: 0\n",
      "-------epoch  9 -------\n",
      "train set Loss: 0.004198072012513876\n",
      "val set Loss: 0\n",
      "-------epoch  10 -------\n",
      "train set Loss: 0.0018835155060514808\n",
      "val set Loss: 0\n",
      "-------epoch  11 -------\n",
      "train set Loss: 0.007888476364314556\n",
      "val set Loss: 0\n",
      "-------epoch  12 -------\n",
      "train set Loss: 0.035343583673238754\n",
      "val set Loss: 0\n",
      "-------epoch  13 -------\n",
      "train set Loss: 0.05106653645634651\n",
      "val set Loss: 0\n",
      "-------epoch  14 -------\n",
      "train set Loss: 0.042147446423769\n",
      "val set Loss: 0\n",
      "-------epoch  15 -------\n",
      "train set Loss: 0.031370338052511215\n",
      "val set Loss: 0\n",
      "-------epoch  16 -------\n",
      "train set Loss: 0.03251371905207634\n",
      "val set Loss: 0\n",
      "-------epoch  17 -------\n",
      "train set Loss: 0.025173872709274292\n",
      "val set Loss: 0\n",
      "-------epoch  18 -------\n",
      "train set Loss: 0.005085432436317205\n",
      "val set Loss: 0\n",
      "-------epoch  19 -------\n",
      "train set Loss: 0.0025220096576958895\n",
      "val set Loss: 0\n",
      "-------epoch  20 -------\n",
      "train set Loss: 0.002728432882577181\n",
      "val set Loss: 0\n",
      "-------epoch  21 -------\n",
      "train set Loss: 0.0024392735213041306\n",
      "val set Loss: 0\n",
      "-------epoch  22 -------\n",
      "train set Loss: 0.005985201336443424\n",
      "val set Loss: 0\n",
      "-------epoch  23 -------\n",
      "train set Loss: 0.0037776087410748005\n",
      "val set Loss: 0\n",
      "-------epoch  24 -------\n",
      "train set Loss: 0.0421111173927784\n",
      "val set Loss: 0\n",
      "-------epoch  25 -------\n",
      "train set Loss: 0.014139569364488125\n",
      "val set Loss: 0\n",
      "-------epoch  26 -------\n",
      "train set Loss: 0.0011365554528310895\n",
      "val set Loss: 0\n",
      "-------epoch  27 -------\n",
      "train set Loss: 0.013676512986421585\n",
      "val set Loss: 0\n",
      "-------epoch  28 -------\n",
      "train set Loss: 0.017100859433412552\n",
      "val set Loss: 0\n",
      "-------epoch  29 -------\n",
      "train set Loss: 0.004220127128064632\n",
      "val set Loss: 0\n",
      "-------epoch  30 -------\n",
      "train set Loss: 0.0023055358324199915\n",
      "val set Loss: 0\n",
      "-------epoch  31 -------\n",
      "train set Loss: 0.0032526282593607903\n",
      "val set Loss: 0\n",
      "-------epoch  32 -------\n",
      "train set Loss: 0.001046302029863\n",
      "val set Loss: 0\n",
      "-------epoch  33 -------\n",
      "train set Loss: 0.0014660496963188052\n",
      "val set Loss: 0\n",
      "-------epoch  34 -------\n",
      "train set Loss: 0.0014173310482874513\n",
      "val set Loss: 0\n",
      "-------epoch  35 -------\n",
      "train set Loss: 0.0006750508910045028\n",
      "val set Loss: 0\n",
      "-------epoch  36 -------\n",
      "train set Loss: 0.0026121458504348993\n",
      "val set Loss: 0\n",
      "-------epoch  37 -------\n",
      "train set Loss: 0.0018088029464706779\n",
      "val set Loss: 0\n",
      "-------epoch  38 -------\n",
      "train set Loss: 0.007808751426637173\n",
      "val set Loss: 0\n",
      "-------epoch  39 -------\n",
      "train set Loss: 0.005836088676005602\n",
      "val set Loss: 0\n",
      "-------epoch  40 -------\n",
      "train set Loss: 0.014951816760003567\n",
      "val set Loss: 0\n",
      "-------epoch  41 -------\n",
      "train set Loss: 0.010657032020390034\n",
      "val set Loss: 0\n",
      "-------epoch  42 -------\n",
      "train set Loss: 0.019010545685887337\n",
      "val set Loss: 0\n",
      "-------epoch  43 -------\n",
      "train set Loss: 0.011444206349551678\n",
      "val set Loss: 0\n",
      "-------epoch  44 -------\n",
      "train set Loss: 0.015048918314278126\n",
      "val set Loss: 0\n",
      "-------epoch  45 -------\n",
      "train set Loss: 0.008211926557123661\n",
      "val set Loss: 0\n",
      "-------epoch  46 -------\n",
      "train set Loss: 0.01141568087041378\n",
      "val set Loss: 0\n",
      "-------epoch  47 -------\n",
      "train set Loss: 0.005983138456940651\n",
      "val set Loss: 0\n",
      "-------epoch  48 -------\n",
      "train set Loss: 0.007857885211706161\n",
      "val set Loss: 0\n",
      "-------epoch  49 -------\n",
      "train set Loss: 0.004330978728830814\n",
      "val set Loss: 0\n",
      "-------epoch  50 -------\n",
      "train set Loss: 0.006994926370680332\n",
      "val set Loss: 0\n",
      "-------epoch  51 -------\n",
      "train set Loss: 0.004234423395246267\n",
      "val set Loss: 0\n",
      "-------epoch  52 -------\n",
      "train set Loss: 0.005488367285579443\n",
      "val set Loss: 0\n",
      "-------epoch  53 -------\n",
      "train set Loss: 0.003828793065622449\n",
      "val set Loss: 0\n",
      "-------epoch  54 -------\n",
      "train set Loss: 0.007678513880819082\n",
      "val set Loss: 0\n",
      "-------epoch  55 -------\n",
      "train set Loss: 0.004326575435698032\n",
      "val set Loss: 0\n",
      "-------epoch  56 -------\n",
      "train set Loss: 0.005705456715077162\n",
      "val set Loss: 0\n",
      "-------epoch  57 -------\n",
      "train set Loss: 0.0039096251130104065\n",
      "val set Loss: 0\n",
      "-------epoch  58 -------\n",
      "train set Loss: 0.007835048250854015\n",
      "val set Loss: 0\n",
      "-------epoch  59 -------\n",
      "train set Loss: 0.00401314627379179\n",
      "val set Loss: 0\n",
      "-------epoch  60 -------\n",
      "train set Loss: 0.0060456921346485615\n",
      "val set Loss: 0\n",
      "-------epoch  61 -------\n",
      "train set Loss: 0.0034122320357710123\n",
      "val set Loss: 0\n",
      "-------epoch  62 -------\n",
      "train set Loss: 0.0066966647282242775\n",
      "val set Loss: 0\n",
      "-------epoch  63 -------\n",
      "train set Loss: 0.003937034867703915\n",
      "val set Loss: 0\n",
      "-------epoch  64 -------\n",
      "train set Loss: 0.005717235151678324\n",
      "val set Loss: 0\n",
      "-------epoch  65 -------\n",
      "train set Loss: 0.003288750536739826\n",
      "val set Loss: 0\n",
      "-------epoch  66 -------\n",
      "train set Loss: 0.007310158107429743\n",
      "val set Loss: 0\n",
      "-------epoch  67 -------\n",
      "train set Loss: 0.004232569597661495\n",
      "val set Loss: 0\n",
      "-------epoch  68 -------\n",
      "train set Loss: 0.006526296492666006\n",
      "val set Loss: 0\n",
      "-------epoch  69 -------\n",
      "train set Loss: 0.003129510907456279\n",
      "val set Loss: 0\n",
      "-------epoch  70 -------\n",
      "train set Loss: 0.007186335977166891\n",
      "val set Loss: 0\n",
      "-------epoch  71 -------\n",
      "train set Loss: 0.004863905254751444\n",
      "val set Loss: 0\n",
      "-------epoch  72 -------\n",
      "train set Loss: 0.007432262878865004\n",
      "val set Loss: 0\n",
      "-------epoch  73 -------\n",
      "train set Loss: 0.003215440548956394\n",
      "val set Loss: 0\n",
      "-------epoch  74 -------\n",
      "train set Loss: 0.0074064964428544044\n",
      "val set Loss: 0\n",
      "-------epoch  75 -------\n",
      "train set Loss: 0.004516796208918095\n",
      "val set Loss: 0\n",
      "-------epoch  76 -------\n",
      "train set Loss: 0.006772281602025032\n",
      "val set Loss: 0\n",
      "-------epoch  77 -------\n",
      "train set Loss: 0.0036676537711173296\n",
      "val set Loss: 0\n",
      "-------epoch  78 -------\n",
      "train set Loss: 0.007027970626950264\n",
      "val set Loss: 0\n",
      "-------epoch  79 -------\n",
      "train set Loss: 0.0038888140115886927\n",
      "val set Loss: 0\n",
      "-------epoch  80 -------\n",
      "train set Loss: 0.006908142473548651\n",
      "val set Loss: 0\n",
      "-------epoch  81 -------\n",
      "train set Loss: 0.0035082942340523005\n",
      "val set Loss: 0\n",
      "-------epoch  82 -------\n",
      "train set Loss: 0.008092875592410564\n",
      "val set Loss: 0\n",
      "-------epoch  83 -------\n",
      "train set Loss: 0.0063875154592096806\n",
      "val set Loss: 0\n",
      "-------epoch  84 -------\n",
      "train set Loss: 0.011646677739918232\n",
      "val set Loss: 0\n",
      "-------epoch  85 -------\n",
      "train set Loss: 0.001016837079077959\n",
      "val set Loss: 0\n",
      "-------epoch  86 -------\n",
      "train set Loss: 0.007704672869294882\n",
      "val set Loss: 0\n",
      "-------epoch  87 -------\n",
      "train set Loss: 0.006874459329992533\n",
      "val set Loss: 0\n",
      "-------epoch  88 -------\n",
      "train set Loss: 0.01254912931472063\n",
      "val set Loss: 0\n",
      "-------epoch  89 -------\n",
      "train set Loss: 0.004929129499942064\n",
      "val set Loss: 0\n",
      "-------epoch  90 -------\n",
      "train set Loss: 0.0053315977565944195\n",
      "val set Loss: 0\n",
      "-------epoch  91 -------\n",
      "train set Loss: 0.0024435301311314106\n",
      "val set Loss: 0\n",
      "-------epoch  92 -------\n",
      "train set Loss: 0.003752676071599126\n",
      "val set Loss: 0\n",
      "-------epoch  93 -------\n",
      "train set Loss: 0.0014169939095154405\n",
      "val set Loss: 0\n",
      "-------epoch  94 -------\n",
      "train set Loss: 0.001800196012482047\n",
      "val set Loss: 0\n",
      "-------epoch  95 -------\n",
      "train set Loss: 0.000768353755120188\n",
      "val set Loss: 0\n",
      "-------epoch  96 -------\n",
      "train set Loss: 0.00175199203658849\n",
      "val set Loss: 0\n",
      "-------epoch  97 -------\n",
      "train set Loss: 0.0006333734490908682\n",
      "val set Loss: 0\n",
      "-------epoch  98 -------\n",
      "train set Loss: 0.0014576148241758347\n",
      "val set Loss: 0\n",
      "-------epoch  99 -------\n",
      "train set Loss: 0.0006561357295140624\n",
      "val set Loss: 0\n",
      "-------epoch  100 -------\n",
      "train set Loss: 0.0010387729853391647\n",
      "val set Loss: 0\n",
      "save model:epoch 100\n",
      "-------epoch  101 -------\n",
      "train set Loss: 0.000710761989466846\n",
      "val set Loss: 0\n",
      "-------epoch  102 -------\n",
      "train set Loss: 0.0012966083595529199\n",
      "val set Loss: 0\n",
      "-------epoch  103 -------\n",
      "train set Loss: 0.0007352359243668616\n",
      "val set Loss: 0\n",
      "-------epoch  104 -------\n",
      "train set Loss: 0.0011983223957940936\n",
      "val set Loss: 0\n",
      "-------epoch  105 -------\n",
      "train set Loss: 0.0007427243399433792\n",
      "val set Loss: 0\n",
      "-------epoch  106 -------\n",
      "train set Loss: 0.001400093431584537\n",
      "val set Loss: 0\n",
      "-------epoch  107 -------\n",
      "train set Loss: 0.0006966629298403859\n",
      "val set Loss: 0\n",
      "-------epoch  108 -------\n",
      "train set Loss: 0.0015447703190147877\n",
      "val set Loss: 0\n",
      "-------epoch  109 -------\n",
      "train set Loss: 0.0006922875763848424\n",
      "val set Loss: 0\n",
      "-------epoch  110 -------\n",
      "train set Loss: 0.0017103733262047172\n",
      "val set Loss: 0\n",
      "-------epoch  111 -------\n",
      "train set Loss: 0.0007630016189068556\n",
      "val set Loss: 0\n",
      "-------epoch  112 -------\n",
      "train set Loss: 0.0022234932985156775\n",
      "val set Loss: 0\n",
      "-------epoch  113 -------\n",
      "train set Loss: 0.0008356315665878356\n",
      "val set Loss: 0\n",
      "-------epoch  114 -------\n",
      "train set Loss: 0.0030515335965901613\n",
      "val set Loss: 0\n",
      "-------epoch  115 -------\n",
      "train set Loss: 0.0017517858650535345\n",
      "val set Loss: 0\n",
      "-------epoch  116 -------\n",
      "train set Loss: 0.010606198571622372\n",
      "val set Loss: 0\n",
      "-------epoch  117 -------\n",
      "train set Loss: 0.0038583327550441027\n",
      "val set Loss: 0\n",
      "-------epoch  118 -------\n",
      "train set Loss: 0.015303049236536026\n",
      "val set Loss: 0\n",
      "-------epoch  119 -------\n",
      "train set Loss: 0.010508136823773384\n",
      "val set Loss: 0\n",
      "-------epoch  120 -------\n",
      "train set Loss: 0.016478098928928375\n",
      "val set Loss: 0\n",
      "-------epoch  121 -------\n",
      "train set Loss: 0.009412740357220173\n",
      "val set Loss: 0\n",
      "-------epoch  122 -------\n",
      "train set Loss: 0.0075997645035386086\n",
      "val set Loss: 0\n",
      "-------epoch  123 -------\n",
      "train set Loss: 0.0030971888918429613\n",
      "val set Loss: 0\n",
      "-------epoch  124 -------\n",
      "train set Loss: 0.0014794559683650732\n",
      "val set Loss: 0\n",
      "-------epoch  125 -------\n",
      "train set Loss: 0.0007855570875108242\n",
      "val set Loss: 0\n",
      "-------epoch  126 -------\n",
      "train set Loss: 0.0013187200529500842\n",
      "val set Loss: 0\n",
      "-------epoch  127 -------\n",
      "train set Loss: 0.003024118021130562\n",
      "val set Loss: 0\n",
      "-------epoch  128 -------\n",
      "train set Loss: 0.001124490750953555\n",
      "val set Loss: 0\n",
      "-------epoch  129 -------\n",
      "train set Loss: 0.002737530507147312\n",
      "val set Loss: 0\n",
      "-------epoch  130 -------\n",
      "train set Loss: 0.005414220038801432\n",
      "val set Loss: 0\n",
      "-------epoch  131 -------\n",
      "train set Loss: 0.0007982487441040576\n",
      "val set Loss: 0\n",
      "-------epoch  132 -------\n",
      "train set Loss: 0.002909229137003422\n",
      "val set Loss: 0\n",
      "-------epoch  133 -------\n",
      "train set Loss: 0.0013040219200775027\n",
      "val set Loss: 0\n",
      "-------epoch  134 -------\n",
      "train set Loss: 0.0016932864673435688\n",
      "val set Loss: 0\n",
      "-------epoch  135 -------\n",
      "train set Loss: 0.0017415181500837207\n",
      "val set Loss: 0\n",
      "-------epoch  136 -------\n",
      "train set Loss: 0.0019579764921218157\n",
      "val set Loss: 0\n",
      "-------epoch  137 -------\n",
      "train set Loss: 0.0019416771829128265\n",
      "val set Loss: 0\n",
      "-------epoch  138 -------\n",
      "train set Loss: 0.002150689484551549\n",
      "val set Loss: 0\n",
      "-------epoch  139 -------\n",
      "train set Loss: 0.0020608205813914537\n",
      "val set Loss: 0\n",
      "-------epoch  140 -------\n",
      "train set Loss: 0.002903224667534232\n",
      "val set Loss: 0\n",
      "-------epoch  141 -------\n",
      "train set Loss: 0.0019392406102269888\n",
      "val set Loss: 0\n",
      "-------epoch  142 -------\n",
      "train set Loss: 0.004626110196113586\n",
      "val set Loss: 0\n",
      "-------epoch  143 -------\n",
      "train set Loss: 0.0021292653400450945\n",
      "val set Loss: 0\n",
      "-------epoch  144 -------\n",
      "train set Loss: 0.004413783550262451\n",
      "val set Loss: 0\n",
      "-------epoch  145 -------\n",
      "train set Loss: 0.0020775289740413427\n",
      "val set Loss: 0\n",
      "-------epoch  146 -------\n",
      "train set Loss: 0.0035620625130832195\n",
      "val set Loss: 0\n",
      "-------epoch  147 -------\n",
      "train set Loss: 0.0010977205820381641\n",
      "val set Loss: 0\n",
      "-------epoch  148 -------\n",
      "train set Loss: 0.0023563678842037916\n",
      "val set Loss: 0\n",
      "-------epoch  149 -------\n",
      "train set Loss: 0.000775607826653868\n",
      "val set Loss: 0\n",
      "-------epoch  150 -------\n",
      "train set Loss: 0.00159228325355798\n",
      "val set Loss: 0\n",
      "-------epoch  151 -------\n",
      "train set Loss: 0.0011023030383512378\n",
      "val set Loss: 0\n",
      "-------epoch  152 -------\n",
      "train set Loss: 0.0016936990432441235\n",
      "val set Loss: 0\n",
      "-------epoch  153 -------\n",
      "train set Loss: 0.0009360898402519524\n",
      "val set Loss: 0\n",
      "-------epoch  154 -------\n",
      "train set Loss: 0.0037518914323300123\n",
      "val set Loss: 0\n",
      "-------epoch  155 -------\n",
      "train set Loss: 0.0008486259612254798\n",
      "val set Loss: 0\n",
      "-------epoch  156 -------\n",
      "train set Loss: 0.008612570352852345\n",
      "val set Loss: 0\n",
      "-------epoch  157 -------\n",
      "train set Loss: 0.0037469130475074053\n",
      "val set Loss: 0\n",
      "-------epoch  158 -------\n",
      "train set Loss: 0.006655924022197723\n",
      "val set Loss: 0\n",
      "-------epoch  159 -------\n",
      "train set Loss: 0.002150834770873189\n",
      "val set Loss: 0\n",
      "-------epoch  160 -------\n",
      "train set Loss: 0.00913924165070057\n",
      "val set Loss: 0\n",
      "-------epoch  161 -------\n",
      "train set Loss: 0.006767190992832184\n",
      "val set Loss: 0\n",
      "-------epoch  162 -------\n",
      "train set Loss: 0.010659919120371342\n",
      "val set Loss: 0\n",
      "-------epoch  163 -------\n",
      "train set Loss: 0.0069611635990440845\n",
      "val set Loss: 0\n",
      "-------epoch  164 -------\n",
      "train set Loss: 0.015363047830760479\n",
      "val set Loss: 0\n",
      "-------epoch  165 -------\n",
      "train set Loss: 0.021504629403352737\n",
      "val set Loss: 0\n",
      "-------epoch  166 -------\n",
      "train set Loss: 0.032853636890649796\n",
      "val set Loss: 0\n",
      "-------epoch  167 -------\n",
      "train set Loss: 0.026836330071091652\n",
      "val set Loss: 0\n",
      "-------epoch  168 -------\n",
      "train set Loss: 0.03208998963236809\n",
      "val set Loss: 0\n",
      "-------epoch  169 -------\n",
      "train set Loss: 0.00863533467054367\n",
      "val set Loss: 0\n",
      "-------epoch  170 -------\n",
      "train set Loss: 0.0032950188033282757\n",
      "val set Loss: 0\n",
      "-------epoch  171 -------\n",
      "train set Loss: 0.0009959156159311533\n",
      "val set Loss: 0\n",
      "-------epoch  172 -------\n",
      "train set Loss: 0.001640765811316669\n",
      "val set Loss: 0\n",
      "-------epoch  173 -------\n",
      "train set Loss: 0.0011777603067457676\n",
      "val set Loss: 0\n",
      "-------epoch  174 -------\n",
      "train set Loss: 0.008197191171348095\n",
      "val set Loss: 0\n",
      "-------epoch  175 -------\n",
      "train set Loss: 0.002103580394759774\n",
      "val set Loss: 0\n",
      "-------epoch  176 -------\n",
      "train set Loss: 0.01500639971345663\n",
      "val set Loss: 0\n",
      "-------epoch  177 -------\n",
      "train set Loss: 0.005421500653028488\n",
      "val set Loss: 0\n",
      "-------epoch  178 -------\n",
      "train set Loss: 0.013335383497178555\n",
      "val set Loss: 0\n",
      "-------epoch  179 -------\n",
      "train set Loss: 0.009312368929386139\n",
      "val set Loss: 0\n",
      "-------epoch  180 -------\n",
      "train set Loss: 0.011459486559033394\n",
      "val set Loss: 0\n",
      "-------epoch  181 -------\n",
      "train set Loss: 0.007671825587749481\n",
      "val set Loss: 0\n",
      "-------epoch  182 -------\n",
      "train set Loss: 0.007695474661886692\n",
      "val set Loss: 0\n",
      "-------epoch  183 -------\n",
      "train set Loss: 0.0046652378514409065\n",
      "val set Loss: 0\n",
      "-------epoch  184 -------\n",
      "train set Loss: 0.00495941424742341\n",
      "val set Loss: 0\n",
      "-------epoch  185 -------\n",
      "train set Loss: 0.003053378313779831\n",
      "val set Loss: 0\n",
      "-------epoch  186 -------\n",
      "train set Loss: 0.003930011764168739\n",
      "val set Loss: 0\n",
      "-------epoch  187 -------\n",
      "train set Loss: 0.0027654210571199656\n",
      "val set Loss: 0\n",
      "-------epoch  188 -------\n",
      "train set Loss: 0.004366504028439522\n",
      "val set Loss: 0\n",
      "-------epoch  189 -------\n",
      "train set Loss: 0.0034382217563688755\n",
      "val set Loss: 0\n",
      "-------epoch  190 -------\n",
      "train set Loss: 0.006101571023464203\n",
      "val set Loss: 0\n",
      "-------epoch  191 -------\n",
      "train set Loss: 0.005045779049396515\n",
      "val set Loss: 0\n",
      "-------epoch  192 -------\n",
      "train set Loss: 0.008952372707426548\n",
      "val set Loss: 0\n",
      "-------epoch  193 -------\n",
      "train set Loss: 0.006739466451108456\n",
      "val set Loss: 0\n",
      "-------epoch  194 -------\n",
      "train set Loss: 0.012037775479257107\n",
      "val set Loss: 0\n",
      "-------epoch  195 -------\n",
      "train set Loss: 0.0057669151574373245\n",
      "val set Loss: 0\n",
      "-------epoch  196 -------\n",
      "train set Loss: 0.011567026376724243\n",
      "val set Loss: 0\n",
      "-------epoch  197 -------\n",
      "train set Loss: 0.0032560834661126137\n",
      "val set Loss: 0\n",
      "-------epoch  198 -------\n",
      "train set Loss: 0.005169686395674944\n",
      "val set Loss: 0\n",
      "-------epoch  199 -------\n",
      "train set Loss: 0.0009699803194962442\n",
      "val set Loss: 0\n",
      "-------epoch  200 -------\n",
      "train set Loss: 0.001082711503840983\n",
      "val set Loss: 0\n",
      "save model:epoch 200\n",
      "-------epoch  201 -------\n",
      "train set Loss: 0.0009429407073184848\n",
      "val set Loss: 0\n",
      "-------epoch  202 -------\n",
      "train set Loss: 0.0008255260763689876\n",
      "val set Loss: 0\n",
      "-------epoch  203 -------\n",
      "train set Loss: 0.0013868347741663456\n",
      "val set Loss: 0\n",
      "-------epoch  204 -------\n",
      "train set Loss: 0.00091089669149369\n",
      "val set Loss: 0\n",
      "-------epoch  205 -------\n",
      "train set Loss: 0.001890971907414496\n",
      "val set Loss: 0\n",
      "-------epoch  206 -------\n",
      "train set Loss: 0.001871011801995337\n",
      "val set Loss: 0\n",
      "-------epoch  207 -------\n",
      "train set Loss: 0.0033142114989459515\n",
      "val set Loss: 0\n",
      "-------epoch  208 -------\n",
      "train set Loss: 0.002359589794650674\n",
      "val set Loss: 0\n",
      "-------epoch  209 -------\n",
      "train set Loss: 0.003915287554264069\n",
      "val set Loss: 0\n",
      "-------epoch  210 -------\n",
      "train set Loss: 0.0010111541487276554\n",
      "val set Loss: 0\n",
      "-------epoch  211 -------\n",
      "train set Loss: 0.0008621547021903098\n",
      "val set Loss: 0\n",
      "-------epoch  212 -------\n",
      "train set Loss: 0.001097515574656427\n",
      "val set Loss: 0\n",
      "-------epoch  213 -------\n",
      "train set Loss: 0.0032279782462865114\n",
      "val set Loss: 0\n",
      "-------epoch  214 -------\n",
      "train set Loss: 0.0010811790125444531\n",
      "val set Loss: 0\n",
      "-------epoch  215 -------\n",
      "train set Loss: 0.0016530861612409353\n",
      "val set Loss: 0\n",
      "-------epoch  216 -------\n",
      "train set Loss: 0.0020563506986945868\n",
      "val set Loss: 0\n",
      "-------epoch  217 -------\n",
      "train set Loss: 0.004230252001434565\n",
      "val set Loss: 0\n",
      "-------epoch  218 -------\n",
      "train set Loss: 0.004947369452565908\n",
      "val set Loss: 0\n",
      "-------epoch  219 -------\n",
      "train set Loss: 0.0008045249851420522\n",
      "val set Loss: 0\n",
      "-------epoch  220 -------\n",
      "train set Loss: 0.001298839459195733\n",
      "val set Loss: 0\n",
      "-------epoch  221 -------\n",
      "train set Loss: 0.002111317589879036\n",
      "val set Loss: 0\n",
      "-------epoch  222 -------\n",
      "train set Loss: 0.002223826479166746\n",
      "val set Loss: 0\n",
      "-------epoch  223 -------\n",
      "train set Loss: 0.0015413084765896201\n",
      "val set Loss: 0\n",
      "-------epoch  224 -------\n",
      "train set Loss: 0.00941065326333046\n",
      "val set Loss: 0\n",
      "-------epoch  225 -------\n",
      "train set Loss: 0.013855740427970886\n",
      "val set Loss: 0\n",
      "-------epoch  226 -------\n",
      "train set Loss: 0.02020452357828617\n",
      "val set Loss: 0\n",
      "-------epoch  227 -------\n",
      "train set Loss: 0.029451290145516396\n",
      "val set Loss: 0\n",
      "-------epoch  228 -------\n",
      "train set Loss: 0.014133763499557972\n",
      "val set Loss: 0\n",
      "-------epoch  229 -------\n",
      "train set Loss: 0.0052896784618496895\n",
      "val set Loss: 0\n",
      "-------epoch  230 -------\n",
      "train set Loss: 0.0011291264090687037\n",
      "val set Loss: 0\n",
      "-------epoch  231 -------\n",
      "train set Loss: 0.0011253505945205688\n",
      "val set Loss: 0\n",
      "-------epoch  232 -------\n",
      "train set Loss: 0.0018351128092035651\n",
      "val set Loss: 0\n",
      "-------epoch  233 -------\n",
      "train set Loss: 0.0028632436878979206\n",
      "val set Loss: 0\n",
      "-------epoch  234 -------\n",
      "train set Loss: 0.0038756215944886208\n",
      "val set Loss: 0\n",
      "-------epoch  235 -------\n",
      "train set Loss: 0.005619236268103123\n",
      "val set Loss: 0\n",
      "-------epoch  236 -------\n",
      "train set Loss: 0.007368097547441721\n",
      "val set Loss: 0\n",
      "-------epoch  237 -------\n",
      "train set Loss: 0.007936698384582996\n",
      "val set Loss: 0\n",
      "-------epoch  238 -------\n",
      "train set Loss: 0.01259241160005331\n",
      "val set Loss: 0\n",
      "-------epoch  239 -------\n",
      "train set Loss: 0.0016352722886949778\n",
      "val set Loss: 0\n",
      "-------epoch  240 -------\n",
      "train set Loss: 0.0021217430476099253\n",
      "val set Loss: 0\n",
      "-------epoch  241 -------\n",
      "train set Loss: 0.0026208278723061085\n",
      "val set Loss: 0\n",
      "-------epoch  242 -------\n",
      "train set Loss: 0.005364621058106422\n",
      "val set Loss: 0\n",
      "-------epoch  243 -------\n",
      "train set Loss: 0.0010207374580204487\n",
      "val set Loss: 0\n",
      "-------epoch  244 -------\n",
      "train set Loss: 0.0008107433677650988\n",
      "val set Loss: 0\n",
      "-------epoch  245 -------\n",
      "train set Loss: 0.001060093636624515\n",
      "val set Loss: 0\n",
      "-------epoch  246 -------\n",
      "train set Loss: 0.0014236199203878641\n",
      "val set Loss: 0\n",
      "-------epoch  247 -------\n",
      "train set Loss: 0.0019231592305004597\n",
      "val set Loss: 0\n",
      "-------epoch  248 -------\n",
      "train set Loss: 0.002753680804744363\n",
      "val set Loss: 0\n",
      "-------epoch  249 -------\n",
      "train set Loss: 0.004022007342427969\n",
      "val set Loss: 0\n",
      "-------epoch  250 -------\n",
      "train set Loss: 0.006120315752923489\n",
      "val set Loss: 0\n",
      "-------epoch  251 -------\n",
      "train set Loss: 0.009269140660762787\n",
      "val set Loss: 0\n",
      "-------epoch  252 -------\n",
      "train set Loss: 0.013843018561601639\n",
      "val set Loss: 0\n",
      "-------epoch  253 -------\n",
      "train set Loss: 0.01935751363635063\n",
      "val set Loss: 0\n",
      "-------epoch  254 -------\n",
      "train set Loss: 0.02329098805785179\n",
      "val set Loss: 0\n",
      "-------epoch  255 -------\n",
      "train set Loss: 0.024129940196871758\n",
      "val set Loss: 0\n",
      "-------epoch  256 -------\n",
      "train set Loss: 0.010772389359772205\n",
      "val set Loss: 0\n",
      "-------epoch  257 -------\n",
      "train set Loss: 0.0038109426386654377\n",
      "val set Loss: 0\n",
      "-------epoch  258 -------\n",
      "train set Loss: 0.0008375762263312936\n",
      "val set Loss: 0\n",
      "-------epoch  259 -------\n",
      "train set Loss: 0.0015574755379930139\n",
      "val set Loss: 0\n",
      "-------epoch  260 -------\n",
      "train set Loss: 0.002204296411946416\n",
      "val set Loss: 0\n",
      "-------epoch  261 -------\n",
      "train set Loss: 0.0007996161002665758\n",
      "val set Loss: 0\n",
      "-------epoch  262 -------\n",
      "train set Loss: 0.0009603243670426309\n",
      "val set Loss: 0\n",
      "-------epoch  263 -------\n",
      "train set Loss: 0.0008090315386652946\n",
      "val set Loss: 0\n",
      "-------epoch  264 -------\n",
      "train set Loss: 0.0007106497068889439\n",
      "val set Loss: 0\n",
      "-------epoch  265 -------\n",
      "train set Loss: 0.001003106590360403\n",
      "val set Loss: 0\n",
      "-------epoch  266 -------\n",
      "train set Loss: 0.0008283272036351264\n",
      "val set Loss: 0\n",
      "-------epoch  267 -------\n",
      "train set Loss: 0.001149053336121142\n",
      "val set Loss: 0\n",
      "-------epoch  268 -------\n",
      "train set Loss: 0.0008620888693258166\n",
      "val set Loss: 0\n",
      "-------epoch  269 -------\n",
      "train set Loss: 0.0008460571407340467\n",
      "val set Loss: 0\n",
      "-------epoch  270 -------\n",
      "train set Loss: 0.0024993238039314747\n",
      "val set Loss: 0\n",
      "-------epoch  271 -------\n",
      "train set Loss: 0.001027097925543785\n",
      "val set Loss: 0\n",
      "-------epoch  272 -------\n",
      "train set Loss: 0.0017806472023949027\n",
      "val set Loss: 0\n",
      "-------epoch  273 -------\n",
      "train set Loss: 0.0007943326490931213\n",
      "val set Loss: 0\n",
      "-------epoch  274 -------\n",
      "train set Loss: 0.0010179259115830064\n",
      "val set Loss: 0\n",
      "-------epoch  275 -------\n",
      "train set Loss: 0.0005984650342725217\n",
      "val set Loss: 0\n",
      "-------epoch  276 -------\n",
      "train set Loss: 0.0006463062018156052\n",
      "val set Loss: 0\n",
      "-------epoch  277 -------\n",
      "train set Loss: 0.0007528309943154454\n",
      "val set Loss: 0\n",
      "-------epoch  278 -------\n",
      "train set Loss: 0.0006111897528171539\n",
      "val set Loss: 0\n",
      "-------epoch  279 -------\n",
      "train set Loss: 0.0007086212281137705\n",
      "val set Loss: 0\n",
      "-------epoch  280 -------\n",
      "train set Loss: 0.0014908708399161696\n",
      "val set Loss: 0\n",
      "-------epoch  281 -------\n",
      "train set Loss: 0.0005785930552519858\n",
      "val set Loss: 0\n",
      "-------epoch  282 -------\n",
      "train set Loss: 0.0022484466899186373\n",
      "val set Loss: 0\n",
      "-------epoch  283 -------\n",
      "train set Loss: 0.0007156164501793683\n",
      "val set Loss: 0\n",
      "-------epoch  284 -------\n",
      "train set Loss: 0.002145924139767885\n",
      "val set Loss: 0\n",
      "-------epoch  285 -------\n",
      "train set Loss: 0.0007501095533370972\n",
      "val set Loss: 0\n",
      "-------epoch  286 -------\n",
      "train set Loss: 0.0012394937220960855\n",
      "val set Loss: 0\n",
      "-------epoch  287 -------\n",
      "train set Loss: 0.0006024549365974963\n",
      "val set Loss: 0\n",
      "-------epoch  288 -------\n",
      "train set Loss: 0.0005802225205115974\n",
      "val set Loss: 0\n",
      "-------epoch  289 -------\n",
      "train set Loss: 0.0018200954655185342\n",
      "val set Loss: 0\n",
      "-------epoch  290 -------\n",
      "train set Loss: 0.0017738408641889691\n",
      "val set Loss: 0\n",
      "-------epoch  291 -------\n",
      "train set Loss: 0.001480927923694253\n",
      "val set Loss: 0\n",
      "-------epoch  292 -------\n",
      "train set Loss: 0.0006948625086806715\n",
      "val set Loss: 0\n",
      "-------epoch  293 -------\n",
      "train set Loss: 0.0024464083835482597\n",
      "val set Loss: 0\n",
      "-------epoch  294 -------\n",
      "train set Loss: 0.0008033571066334844\n",
      "val set Loss: 0\n",
      "-------epoch  295 -------\n",
      "train set Loss: 0.002732518594712019\n",
      "val set Loss: 0\n",
      "-------epoch  296 -------\n",
      "train set Loss: 0.0005901519325561821\n",
      "val set Loss: 0\n",
      "-------epoch  297 -------\n",
      "train set Loss: 0.0041076079942286015\n",
      "val set Loss: 0\n",
      "-------epoch  298 -------\n",
      "train set Loss: 0.000856950762681663\n",
      "val set Loss: 0\n",
      "-------epoch  299 -------\n",
      "train set Loss: 0.007599699776619673\n",
      "val set Loss: 0\n",
      "-------epoch  300 -------\n",
      "train set Loss: 0.0013929089764133096\n",
      "val set Loss: 0\n",
      "save model:epoch 300\n",
      "-------epoch  301 -------\n",
      "train set Loss: 0.009382416494190693\n",
      "val set Loss: 0\n",
      "-------epoch  302 -------\n",
      "train set Loss: 0.001756165991537273\n",
      "val set Loss: 0\n",
      "-------epoch  303 -------\n",
      "train set Loss: 0.008137126453220844\n",
      "val set Loss: 0\n",
      "-------epoch  304 -------\n",
      "train set Loss: 0.001338583999313414\n",
      "val set Loss: 0\n",
      "-------epoch  305 -------\n",
      "train set Loss: 0.005401031579822302\n",
      "val set Loss: 0\n",
      "-------epoch  306 -------\n",
      "train set Loss: 0.0008581485017202795\n",
      "val set Loss: 0\n",
      "-------epoch  307 -------\n",
      "train set Loss: 0.0021865766029804945\n",
      "val set Loss: 0\n",
      "-------epoch  308 -------\n",
      "train set Loss: 0.000570571341086179\n",
      "val set Loss: 0\n",
      "-------epoch  309 -------\n",
      "train set Loss: 0.0007921181968413293\n",
      "val set Loss: 0\n",
      "-------epoch  310 -------\n",
      "train set Loss: 0.000654257251881063\n",
      "val set Loss: 0\n",
      "-------epoch  311 -------\n",
      "train set Loss: 0.0008202947210520506\n",
      "val set Loss: 0\n",
      "-------epoch  312 -------\n",
      "train set Loss: 0.0005280586774460971\n",
      "val set Loss: 0\n",
      "-------epoch  313 -------\n",
      "train set Loss: 0.002299559535458684\n",
      "val set Loss: 0\n",
      "-------epoch  314 -------\n",
      "train set Loss: 0.0010134518379345536\n",
      "val set Loss: 0\n",
      "-------epoch  315 -------\n",
      "train set Loss: 0.0006896048435010016\n",
      "val set Loss: 0\n",
      "-------epoch  316 -------\n",
      "train set Loss: 0.0026715537533164024\n",
      "val set Loss: 0\n",
      "-------epoch  317 -------\n",
      "train set Loss: 0.0006512711406685412\n",
      "val set Loss: 0\n",
      "-------epoch  318 -------\n",
      "train set Loss: 0.0026289180386811495\n",
      "val set Loss: 0\n",
      "-------epoch  319 -------\n",
      "train set Loss: 0.0005232123658061028\n",
      "val set Loss: 0\n",
      "-------epoch  320 -------\n",
      "train set Loss: 0.0017297972226515412\n",
      "val set Loss: 0\n",
      "-------epoch  321 -------\n",
      "train set Loss: 0.0010044582886621356\n",
      "val set Loss: 0\n",
      "-------epoch  322 -------\n",
      "train set Loss: 0.0021953091491013765\n",
      "val set Loss: 0\n",
      "-------epoch  323 -------\n",
      "train set Loss: 0.0077443853951990604\n",
      "val set Loss: 0\n",
      "-------epoch  324 -------\n",
      "train set Loss: 0.0049396902322769165\n",
      "val set Loss: 0\n",
      "-------epoch  325 -------\n",
      "train set Loss: 0.006199497263878584\n",
      "val set Loss: 0\n",
      "-------epoch  326 -------\n",
      "train set Loss: 0.004607211798429489\n",
      "val set Loss: 0\n",
      "-------epoch  327 -------\n",
      "train set Loss: 0.010887009091675282\n",
      "val set Loss: 0\n",
      "-------epoch  328 -------\n",
      "train set Loss: 0.005684568081051111\n",
      "val set Loss: 0\n",
      "-------epoch  329 -------\n",
      "train set Loss: 0.003942664247006178\n",
      "val set Loss: 0\n",
      "-------epoch  330 -------\n",
      "train set Loss: 0.0011610505171120167\n",
      "val set Loss: 0\n",
      "-------epoch  331 -------\n",
      "train set Loss: 0.0005324202356860042\n",
      "val set Loss: 0\n",
      "-------epoch  332 -------\n",
      "train set Loss: 0.0018680696375668049\n",
      "val set Loss: 0\n",
      "-------epoch  333 -------\n",
      "train set Loss: 0.0006884874892421067\n",
      "val set Loss: 0\n",
      "-------epoch  334 -------\n",
      "train set Loss: 0.0014293696731328964\n",
      "val set Loss: 0\n",
      "-------epoch  335 -------\n",
      "train set Loss: 0.0005233646952547133\n",
      "val set Loss: 0\n",
      "-------epoch  336 -------\n",
      "train set Loss: 0.0005827433196827769\n",
      "val set Loss: 0\n",
      "-------epoch  337 -------\n",
      "train set Loss: 0.0020791632123291492\n",
      "val set Loss: 0\n",
      "-------epoch  338 -------\n",
      "train set Loss: 0.0006388889742083848\n",
      "val set Loss: 0\n",
      "-------epoch  339 -------\n",
      "train set Loss: 0.00175986357498914\n",
      "val set Loss: 0\n",
      "-------epoch  340 -------\n",
      "train set Loss: 0.0006642602384090424\n",
      "val set Loss: 0\n",
      "-------epoch  341 -------\n",
      "train set Loss: 0.0023869117721915245\n",
      "val set Loss: 0\n",
      "-------epoch  342 -------\n",
      "train set Loss: 0.0005793074378743768\n",
      "val set Loss: 0\n",
      "-------epoch  343 -------\n",
      "train set Loss: 0.0018480619182810187\n",
      "val set Loss: 0\n",
      "-------epoch  344 -------\n",
      "train set Loss: 0.0005222129402682185\n",
      "val set Loss: 0\n",
      "-------epoch  345 -------\n",
      "train set Loss: 0.0020939947571605444\n",
      "val set Loss: 0\n",
      "-------epoch  346 -------\n",
      "train set Loss: 0.0007146177813410759\n",
      "val set Loss: 0\n",
      "-------epoch  347 -------\n",
      "train set Loss: 0.0039149317890405655\n",
      "val set Loss: 0\n",
      "-------epoch  348 -------\n",
      "train set Loss: 0.0013795302947983146\n",
      "val set Loss: 0\n",
      "-------epoch  349 -------\n",
      "train set Loss: 0.007497945800423622\n",
      "val set Loss: 0\n",
      "-------epoch  350 -------\n",
      "train set Loss: 0.0014761524507775903\n",
      "val set Loss: 0\n",
      "-------epoch  351 -------\n",
      "train set Loss: 0.002080247737467289\n",
      "val set Loss: 0\n",
      "-------epoch  352 -------\n",
      "train set Loss: 0.0005812387098558247\n",
      "val set Loss: 0\n",
      "-------epoch  353 -------\n",
      "train set Loss: 0.0008259851601906121\n",
      "val set Loss: 0\n",
      "-------epoch  354 -------\n",
      "train set Loss: 0.0031652890611439943\n",
      "val set Loss: 0\n",
      "-------epoch  355 -------\n",
      "train set Loss: 0.0008677357109263539\n",
      "val set Loss: 0\n",
      "-------epoch  356 -------\n",
      "train set Loss: 0.0015023580053821206\n",
      "val set Loss: 0\n",
      "-------epoch  357 -------\n",
      "train set Loss: 0.0014773880830034614\n",
      "val set Loss: 0\n",
      "-------epoch  358 -------\n",
      "train set Loss: 0.003306441940367222\n",
      "val set Loss: 0\n",
      "-------epoch  359 -------\n",
      "train set Loss: 0.0006841553840786219\n",
      "val set Loss: 0\n",
      "-------epoch  360 -------\n",
      "train set Loss: 0.0007154729682952166\n",
      "val set Loss: 0\n",
      "-------epoch  361 -------\n",
      "train set Loss: 0.0009495675913058221\n",
      "val set Loss: 0\n",
      "-------epoch  362 -------\n",
      "train set Loss: 0.004269433673471212\n",
      "val set Loss: 0\n",
      "-------epoch  363 -------\n",
      "train set Loss: 0.0022837528958916664\n",
      "val set Loss: 0\n",
      "-------epoch  364 -------\n",
      "train set Loss: 0.010093377903103828\n",
      "val set Loss: 0\n",
      "-------epoch  365 -------\n",
      "train set Loss: 0.005601900164037943\n",
      "val set Loss: 0\n",
      "-------epoch  366 -------\n",
      "train set Loss: 0.0006289840675890446\n",
      "val set Loss: 0\n",
      "-------epoch  367 -------\n",
      "train set Loss: 0.0005641862517222762\n",
      "val set Loss: 0\n",
      "-------epoch  368 -------\n",
      "train set Loss: 0.0017314305296167731\n",
      "val set Loss: 0\n",
      "-------epoch  369 -------\n",
      "train set Loss: 0.0007663957076147199\n",
      "val set Loss: 0\n",
      "-------epoch  370 -------\n",
      "train set Loss: 0.002282292814925313\n",
      "val set Loss: 0\n",
      "-------epoch  371 -------\n",
      "train set Loss: 0.001045435550622642\n",
      "val set Loss: 0\n",
      "-------epoch  372 -------\n",
      "train set Loss: 0.0005561789148487151\n",
      "val set Loss: 0\n",
      "-------epoch  373 -------\n",
      "train set Loss: 0.0007456776802428067\n",
      "val set Loss: 0\n",
      "-------epoch  374 -------\n",
      "train set Loss: 0.0007074408931657672\n",
      "val set Loss: 0\n",
      "-------epoch  375 -------\n",
      "train set Loss: 0.0005594342364929616\n",
      "val set Loss: 0\n",
      "-------epoch  376 -------\n",
      "train set Loss: 0.004689155612140894\n",
      "val set Loss: 0\n",
      "-------epoch  377 -------\n",
      "train set Loss: 0.0037000146694481373\n",
      "val set Loss: 0\n",
      "-------epoch  378 -------\n",
      "train set Loss: 0.007153662387281656\n",
      "val set Loss: 0\n",
      "-------epoch  379 -------\n",
      "train set Loss: 0.004881518427282572\n",
      "val set Loss: 0\n",
      "-------epoch  380 -------\n",
      "train set Loss: 0.0022993157617747784\n",
      "val set Loss: 0\n",
      "-------epoch  381 -------\n",
      "train set Loss: 0.0005636232672259212\n",
      "val set Loss: 0\n",
      "-------epoch  382 -------\n",
      "train set Loss: 0.0008603648166172206\n",
      "val set Loss: 0\n",
      "-------epoch  383 -------\n",
      "train set Loss: 0.0033351511228829622\n",
      "val set Loss: 0\n",
      "-------epoch  384 -------\n",
      "train set Loss: 0.0072670429944992065\n",
      "val set Loss: 0\n",
      "-------epoch  385 -------\n",
      "train set Loss: 0.00729836942628026\n",
      "val set Loss: 0\n",
      "-------epoch  386 -------\n",
      "train set Loss: 0.00474052969366312\n",
      "val set Loss: 0\n",
      "-------epoch  387 -------\n",
      "train set Loss: 0.008070682175457478\n",
      "val set Loss: 0\n",
      "-------epoch  388 -------\n",
      "train set Loss: 0.001665608724579215\n",
      "val set Loss: 0\n",
      "-------epoch  389 -------\n",
      "train set Loss: 0.001647076103836298\n",
      "val set Loss: 0\n",
      "-------epoch  390 -------\n",
      "train set Loss: 0.0012221955694258213\n",
      "val set Loss: 0\n",
      "-------epoch  391 -------\n",
      "train set Loss: 0.009408214129507542\n",
      "val set Loss: 0\n",
      "-------epoch  392 -------\n",
      "train set Loss: 0.0048254565335810184\n",
      "val set Loss: 0\n",
      "-------epoch  393 -------\n",
      "train set Loss: 0.0020660359878093004\n",
      "val set Loss: 0\n",
      "-------epoch  394 -------\n",
      "train set Loss: 0.003032579319551587\n",
      "val set Loss: 0\n",
      "-------epoch  395 -------\n",
      "train set Loss: 0.002578419167548418\n",
      "val set Loss: 0\n",
      "-------epoch  396 -------\n",
      "train set Loss: 0.0027216209564357996\n",
      "val set Loss: 0\n",
      "-------epoch  397 -------\n",
      "train set Loss: 0.006486940663307905\n",
      "val set Loss: 0\n",
      "-------epoch  398 -------\n",
      "train set Loss: 0.006349725183099508\n",
      "val set Loss: 0\n",
      "-------epoch  399 -------\n",
      "train set Loss: 0.00526008615270257\n",
      "val set Loss: 0\n",
      "-------epoch  400 -------\n",
      "train set Loss: 0.002543540671467781\n",
      "val set Loss: 0\n",
      "save model:epoch 400\n",
      "-------epoch  401 -------\n",
      "train set Loss: 0.001719549298286438\n",
      "val set Loss: 0\n",
      "-------epoch  402 -------\n",
      "train set Loss: 0.0005917339585721493\n",
      "val set Loss: 0\n",
      "-------epoch  403 -------\n",
      "train set Loss: 0.000560094544198364\n",
      "val set Loss: 0\n",
      "-------epoch  404 -------\n",
      "train set Loss: 0.0006617431645281613\n",
      "val set Loss: 0\n",
      "-------epoch  405 -------\n",
      "train set Loss: 0.0005782491061836481\n",
      "val set Loss: 0\n",
      "-------epoch  406 -------\n",
      "train set Loss: 0.0013292330550029874\n",
      "val set Loss: 0\n",
      "-------epoch  407 -------\n",
      "train set Loss: 0.0005158568965271115\n",
      "val set Loss: 0\n",
      "-------epoch  408 -------\n",
      "train set Loss: 0.0023416250478476286\n",
      "val set Loss: 0\n",
      "-------epoch  409 -------\n",
      "train set Loss: 0.0005939223919995129\n",
      "val set Loss: 0\n",
      "-------epoch  410 -------\n",
      "train set Loss: 0.002884805668145418\n",
      "val set Loss: 0\n",
      "-------epoch  411 -------\n",
      "train set Loss: 0.0011688319500535727\n",
      "val set Loss: 0\n",
      "-------epoch  412 -------\n",
      "train set Loss: 0.004086462315171957\n",
      "val set Loss: 0\n",
      "-------epoch  413 -------\n",
      "train set Loss: 0.0026547389570623636\n",
      "val set Loss: 0\n",
      "-------epoch  414 -------\n",
      "train set Loss: 0.005479308310896158\n",
      "val set Loss: 0\n",
      "-------epoch  415 -------\n",
      "train set Loss: 0.005909088999032974\n",
      "val set Loss: 0\n",
      "-------epoch  416 -------\n",
      "train set Loss: 0.00567060150206089\n",
      "val set Loss: 0\n",
      "-------epoch  417 -------\n",
      "train set Loss: 0.009770256467163563\n",
      "val set Loss: 0\n",
      "-------epoch  418 -------\n",
      "train set Loss: 0.007676243782043457\n",
      "val set Loss: 0\n",
      "-------epoch  419 -------\n",
      "train set Loss: 0.00697251595556736\n",
      "val set Loss: 0\n",
      "-------epoch  420 -------\n",
      "train set Loss: 0.002106056082993746\n",
      "val set Loss: 0\n",
      "-------epoch  421 -------\n",
      "train set Loss: 0.0005490322364494205\n",
      "val set Loss: 0\n",
      "-------epoch  422 -------\n",
      "train set Loss: 0.0008249266538769007\n",
      "val set Loss: 0\n",
      "-------epoch  423 -------\n",
      "train set Loss: 0.0024387550074607134\n",
      "val set Loss: 0\n",
      "-------epoch  424 -------\n",
      "train set Loss: 0.0005189051735214889\n",
      "val set Loss: 0\n",
      "-------epoch  425 -------\n",
      "train set Loss: 0.000838037405628711\n",
      "val set Loss: 0\n",
      "-------epoch  426 -------\n",
      "train set Loss: 0.0004996008356101811\n",
      "val set Loss: 0\n",
      "-------epoch  427 -------\n",
      "train set Loss: 0.00114294677041471\n",
      "val set Loss: 0\n",
      "-------epoch  428 -------\n",
      "train set Loss: 0.0005084345466457307\n",
      "val set Loss: 0\n",
      "-------epoch  429 -------\n",
      "train set Loss: 0.0014065062860026956\n",
      "val set Loss: 0\n",
      "-------epoch  430 -------\n",
      "train set Loss: 0.000666969979647547\n",
      "val set Loss: 0\n",
      "-------epoch  431 -------\n",
      "train set Loss: 0.0022651494946330786\n",
      "val set Loss: 0\n",
      "-------epoch  432 -------\n",
      "train set Loss: 0.000508641533087939\n",
      "val set Loss: 0\n",
      "-------epoch  433 -------\n",
      "train set Loss: 0.0005462096887640655\n",
      "val set Loss: 0\n",
      "-------epoch  434 -------\n",
      "train set Loss: 0.0005814257892780006\n",
      "val set Loss: 0\n",
      "-------epoch  435 -------\n",
      "train set Loss: 0.0005210740491747856\n",
      "val set Loss: 0\n",
      "-------epoch  436 -------\n",
      "train set Loss: 0.0009173352736979723\n",
      "val set Loss: 0\n",
      "-------epoch  437 -------\n",
      "train set Loss: 0.0008968159672804177\n",
      "val set Loss: 0\n",
      "-------epoch  438 -------\n",
      "train set Loss: 0.001148992800153792\n",
      "val set Loss: 0\n",
      "-------epoch  439 -------\n",
      "train set Loss: 0.0017060256795957685\n",
      "val set Loss: 0\n",
      "-------epoch  440 -------\n",
      "train set Loss: 0.003128903219476342\n",
      "val set Loss: 0\n",
      "-------epoch  441 -------\n",
      "train set Loss: 0.006370401941239834\n",
      "val set Loss: 0\n",
      "-------epoch  442 -------\n",
      "train set Loss: 0.009467557072639465\n",
      "val set Loss: 0\n",
      "-------epoch  443 -------\n",
      "train set Loss: 0.009151803329586983\n",
      "val set Loss: 0\n",
      "-------epoch  444 -------\n",
      "train set Loss: 0.0018608623649924994\n",
      "val set Loss: 0\n",
      "-------epoch  445 -------\n",
      "train set Loss: 0.0009885473409667611\n",
      "val set Loss: 0\n",
      "-------epoch  446 -------\n",
      "train set Loss: 0.0007864607614465058\n",
      "val set Loss: 0\n",
      "-------epoch  447 -------\n",
      "train set Loss: 0.0011822839733213186\n",
      "val set Loss: 0\n",
      "-------epoch  448 -------\n",
      "train set Loss: 0.0031220701057463884\n",
      "val set Loss: 0\n",
      "-------epoch  449 -------\n",
      "train set Loss: 0.0027474837843328714\n",
      "val set Loss: 0\n",
      "-------epoch  450 -------\n",
      "train set Loss: 0.00465119956061244\n",
      "val set Loss: 0\n",
      "-------epoch  451 -------\n",
      "train set Loss: 0.003011295571923256\n",
      "val set Loss: 0\n",
      "-------epoch  452 -------\n",
      "train set Loss: 0.001498646684922278\n",
      "val set Loss: 0\n",
      "-------epoch  453 -------\n",
      "train set Loss: 0.000512812752276659\n",
      "val set Loss: 0\n",
      "-------epoch  454 -------\n",
      "train set Loss: 0.0004835819127038121\n",
      "val set Loss: 0\n",
      "-------epoch  455 -------\n",
      "train set Loss: 0.001811642199754715\n",
      "val set Loss: 0\n",
      "-------epoch  456 -------\n",
      "train set Loss: 0.0004988245782442391\n",
      "val set Loss: 0\n",
      "-------epoch  457 -------\n",
      "train set Loss: 0.001402404741384089\n",
      "val set Loss: 0\n",
      "-------epoch  458 -------\n",
      "train set Loss: 0.0005186900380067527\n",
      "val set Loss: 0\n",
      "-------epoch  459 -------\n",
      "train set Loss: 0.0011772233992815018\n",
      "val set Loss: 0\n",
      "-------epoch  460 -------\n",
      "train set Loss: 0.0005559824057854712\n",
      "val set Loss: 0\n",
      "-------epoch  461 -------\n",
      "train set Loss: 0.0009633232257328928\n",
      "val set Loss: 0\n",
      "-------epoch  462 -------\n",
      "train set Loss: 0.0005518410471267998\n",
      "val set Loss: 0\n",
      "-------epoch  463 -------\n",
      "train set Loss: 0.0006982745253480971\n",
      "val set Loss: 0\n",
      "-------epoch  464 -------\n",
      "train set Loss: 0.0007399455644190311\n",
      "val set Loss: 0\n",
      "-------epoch  465 -------\n",
      "train set Loss: 0.0005482154083438218\n",
      "val set Loss: 0\n",
      "-------epoch  466 -------\n",
      "train set Loss: 0.003453835379332304\n",
      "val set Loss: 0\n",
      "-------epoch  467 -------\n",
      "train set Loss: 0.0028969240374863148\n",
      "val set Loss: 0\n",
      "-------epoch  468 -------\n",
      "train set Loss: 0.001813944661989808\n",
      "val set Loss: 0\n",
      "-------epoch  469 -------\n",
      "train set Loss: 0.0005944778094999492\n",
      "val set Loss: 0\n",
      "-------epoch  470 -------\n",
      "train set Loss: 0.00067075778497383\n",
      "val set Loss: 0\n",
      "-------epoch  471 -------\n",
      "train set Loss: 0.0021395618095993996\n",
      "val set Loss: 0\n",
      "-------epoch  472 -------\n",
      "train set Loss: 0.005392943974584341\n",
      "val set Loss: 0\n",
      "-------epoch  473 -------\n",
      "train set Loss: 0.006592495832592249\n",
      "val set Loss: 0\n",
      "-------epoch  474 -------\n",
      "train set Loss: 0.004827436991035938\n",
      "val set Loss: 0\n",
      "-------epoch  475 -------\n",
      "train set Loss: 0.0006349114119075239\n",
      "val set Loss: 0\n",
      "-------epoch  476 -------\n",
      "train set Loss: 0.0014323416398838162\n",
      "val set Loss: 0\n",
      "-------epoch  477 -------\n",
      "train set Loss: 0.0010842906776815653\n",
      "val set Loss: 0\n",
      "-------epoch  478 -------\n",
      "train set Loss: 0.0018484836909919977\n",
      "val set Loss: 0\n",
      "-------epoch  479 -------\n",
      "train set Loss: 0.001100356807000935\n",
      "val set Loss: 0\n",
      "-------epoch  480 -------\n",
      "train set Loss: 0.007445989642292261\n",
      "val set Loss: 0\n",
      "-------epoch  481 -------\n",
      "train set Loss: 0.0020642688032239676\n",
      "val set Loss: 0\n",
      "-------epoch  482 -------\n",
      "train set Loss: 0.001133429235778749\n",
      "val set Loss: 0\n",
      "-------epoch  483 -------\n",
      "train set Loss: 0.0008500847034156322\n",
      "val set Loss: 0\n",
      "-------epoch  484 -------\n",
      "train set Loss: 0.0008310126140713692\n",
      "val set Loss: 0\n",
      "-------epoch  485 -------\n",
      "train set Loss: 0.0032664446625858545\n",
      "val set Loss: 0\n",
      "-------epoch  486 -------\n",
      "train set Loss: 0.0006273454055190086\n",
      "val set Loss: 0\n",
      "-------epoch  487 -------\n",
      "train set Loss: 0.0009485558839514852\n",
      "val set Loss: 0\n",
      "-------epoch  488 -------\n",
      "train set Loss: 0.0005804323591291904\n",
      "val set Loss: 0\n",
      "-------epoch  489 -------\n",
      "train set Loss: 0.0006936519057489932\n",
      "val set Loss: 0\n",
      "-------epoch  490 -------\n",
      "train set Loss: 0.0006983604398556054\n",
      "val set Loss: 0\n",
      "-------epoch  491 -------\n",
      "train set Loss: 0.0007502283551730216\n",
      "val set Loss: 0\n",
      "-------epoch  492 -------\n",
      "train set Loss: 0.0007707636104896665\n",
      "val set Loss: 0\n",
      "-------epoch  493 -------\n",
      "train set Loss: 0.0006885597831569612\n",
      "val set Loss: 0\n",
      "-------epoch  494 -------\n",
      "train set Loss: 0.0006220503128133714\n",
      "val set Loss: 0\n",
      "-------epoch  495 -------\n",
      "train set Loss: 0.0007858990575186908\n",
      "val set Loss: 0\n",
      "-------epoch  496 -------\n",
      "train set Loss: 0.0005961081478744745\n",
      "val set Loss: 0\n",
      "-------epoch  497 -------\n",
      "train set Loss: 0.0012087153736501932\n",
      "val set Loss: 0\n",
      "-------epoch  498 -------\n",
      "train set Loss: 0.0006142099155113101\n",
      "val set Loss: 0\n",
      "-------epoch  499 -------\n",
      "train set Loss: 0.002310197101905942\n",
      "val set Loss: 0\n",
      "-------epoch  500 -------\n",
      "train set Loss: 0.0007854883442632854\n",
      "val set Loss: 0\n",
      "save model:epoch 500\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "if __name__ == \"__main__\":\n",
    "    # checking if GPU is available\n",
    "    device = torch.device(\"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "\n",
    "    # 构建模型\n",
    "    model = LSTMModel(input_size=input_size,\n",
    "                      hidden_size=hidden_size,\n",
    "                      output_size=output_size,\n",
    "                      num_layers = num_layers)\n",
    "    model = model.to(device) # lstm doesnt work on gpu?\n",
    "    # dataset\n",
    "    train_dataset = LoadDataset(train_directory, seq_len=seq_len, features=features)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    #verify\n",
    "    verify_dataset = LoadDataset(verify_directory, seq_len=seq_len, features=features)\n",
    "    verify_dataloader = DataLoader(verify_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 储存路径\n",
    "    work_dir = './LSTM'\n",
    "    # 添加tensorboard\n",
    "    writer = SummaryWriter(\"{}/logs\".format(work_dir))\n",
    "\n",
    "    # model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    checkponits = epoch // 5\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(epoch):\n",
    "        epoch = epoch + 1 # from 1\n",
    "        print(\"-------epoch  {} -------\".format(epoch))\n",
    "        # 训练步骤\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for train_data, train_data_real in train_dataloader:\n",
    "            train_data = torch.squeeze(train_data).to(device)\n",
    "            train_data_real = torch.squeeze(train_data_real).to(device)\n",
    "\n",
    "            output = model(train_data)\n",
    "            output = torch.squeeze(output)\n",
    "            loss = criterion(output, train_data_real)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        print(\"train set Loss: {}\".format(loss.item()))\n",
    "\n",
    "        # 测试步骤\n",
    "        model.eval()\n",
    "        total_verify_loss = 0\n",
    "        with torch.no_grad():#用于在推断或验证阶段，当不需要计算梯度时，以提高效率和减少内存占用\n",
    "            for verify_data, verify_data_real in verify_dataloader:\n",
    "                verify_data = torch.squeeze(verify_data).to(device)\n",
    "                verify_data_real = torch.squeeze(verify_data_real).to(device)\n",
    "\n",
    "                verify_output = model(verify_data)\n",
    "                verify_loss = criterion(verify_output, verify_data_real)\n",
    "                total_verify_loss = total_verify_loss + verify_loss.item()\n",
    "        print(\"val set Loss: {}\".format(total_verify_loss))\n",
    "\n",
    "        # save checkpoint\n",
    "        if epoch % checkponits == 0:\n",
    "            torch.save(model.state_dict(), save_path[:-4]+str(epoch)+'.pth')\n",
    "            print(\"save model:epoch {}\".format(epoch))\n",
    "    # save last1\n",
    "    torch.save(model.state_dict(), save_path[:-4]+'last.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T17:24:11.155876100Z",
     "start_time": "2024-03-06T15:00:21.226296200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.eval and plot comparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Discharge B0018')"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 800x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_battery_id = 'B0018'\n",
    "# save_path = save_path\n",
    "save_path = 'seq50_500.pth'\n",
    "\n",
    "test_battery_path = test_directory+ test_battery_id + '.mat'\n",
    "test_data,test_data_real = data_loader(test_battery_path,seq_len,features)\n",
    "# load\n",
    "lstm = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "lstm.load_state_dict(torch.load(save_path))\n",
    "lstm.eval()\n",
    "output = lstm(test_data)\n",
    "predicted_data = output.detach().numpy().reshape(-1)\n",
    "print(predicted_data.shape)\n",
    "\n",
    "\n",
    "# draw\n",
    "origin_data = norm_data(test_battery_path) # dictionary\n",
    "sns.set_style(\"white\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "#Draw\n",
    "plt.plot([i for i in range(len(origin_data['SOH']))], origin_data['SOH'], label='SOH')\n",
    "# predicted_data = np.concatenate((origin_data['SOH'][0:seq_len],predicted_data), axis = 0)\n",
    "plt.plot([i for i in range(seq_len,len(origin_data['SOH']))], predicted_data, label='Predicted SOH')\n",
    "plt.plot([0.,len(origin_data['SOH'])], [0.70, 0.70], label='Threshold')\n",
    "\n",
    "# make x-axis ticks legible\n",
    "adf = plt.gca().get_xaxis().get_major_formatter()\n",
    "plt.xlabel('cycle')\n",
    "plt.ylabel('SOH')\n",
    "plt.title('Discharge {}'.format(test_battery_id))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T17:25:54.961944500Z",
     "start_time": "2024-03-06T17:25:52.631310400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
