{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-01T10:13:39.364975100Z",
     "start_time": "2024-03-01T10:13:24.553043200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3.8\\envs\\battery\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.hyperparameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# hyper\n",
    "features = ['SOH','voltage_measured', 'current_measured',\n",
    "            'temperature_measured', 'time']\n",
    "batch_size = 1  # 1*len(every_file)\n",
    "input_size = len(features)\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "seq_len = 20   # 预测序列长度\n",
    "epoch = 1000   # 1*len(train_directory)\n",
    "learning_rate = 0.001  # upgrade to adaptive lr?\n",
    "\n",
    "save_path = 'seq{}_.pth'.format(str(seq_len))  # model path\n",
    "train_directory = '../datasets/train/'\n",
    "val_directory = '../datasets/val/'\n",
    "test_directory = '../datasets/alldataset/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T10:14:12.590019600Z",
     "start_time": "2024-03-01T10:14:12.564010300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "# checking if GPU is available\n",
    "device = torch.device(\"cpu\")\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "\n",
    "# 构建模型\n",
    "model = LSTMModel(input_size=input_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  output_size=output_size,\n",
    "                  num_layers = num_layers)\n",
    "model = model.to(device) # lstm doesnt work on gpu?\n",
    "# train\n",
    "train_dataset = LoadDataset(train_directory, seq_len=seq_len, features=features)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# validation\n",
    "val_dataset = LoadDataset(val_directory, seq_len=seq_len, features=features)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 储存路径\n",
    "work_dir = './LSTM'\n",
    "# 添加tensorboard\n",
    "writer = SummaryWriter(\"{}/logs\".format(work_dir))\n",
    "\n",
    "# model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# checkponits = epoch // 5\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epoch):\n",
    "    print(\"-------epoch  {} -------\".format(epoch))\n",
    "    # 训练步骤\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for train_data, train_data_real in train_dataloader:\n",
    "        train_data = torch.squeeze(train_data).to(device)\n",
    "        train_data_real = torch.squeeze(train_data_real).to(device)\n",
    "\n",
    "        output = model(train_data)\n",
    "        output = torch.squeeze(output)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = criterion(output, train_data_real)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader)\n",
    "    print(\"train set Loss: {}\".format(avg_train_loss)) # 出现nan可能是seq_len太长了,有些数据集比seq_len短\n",
    "\n",
    "    # 测试步骤\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():#用于在推断或验证阶段，当不需要计算梯度时，以提高效率和减少内存占用\n",
    "        for val_data, val_data_real in val_dataloader:\n",
    "            val_data = torch.squeeze(val_data).to(device)\n",
    "            val_data_real = torch.squeeze(val_data_real).to(device)\n",
    "\n",
    "            val_output = model(val_data)\n",
    "            val_output = torch.squeeze(val_output)\n",
    "            val_loss = criterion(val_output, val_data_real)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss/len(val_dataloader)\n",
    "    print(\"val set Loss: {}\".format(avg_val_loss))\n",
    "\n",
    "    # save checkpoint\n",
    "    # if epochs % checkponits == 0:\n",
    "    if avg_train_loss < 0.0001  and avg_val_loss.item()< 0.0001 :\n",
    "        torch.save(model.state_dict(), str('EX_')+save_path[:-4]+str(epoch)+'.pth')\n",
    "        print(\"save model:epoch {}\".format(epoch))\n",
    "    if avg_train_loss < 0.0005  and avg_val_loss.item()< 0.0005 :\n",
    "        torch.save(model.state_dict(), str('A_')+save_path[:-4]+str(epoch)+'.pth')\n",
    "        print(\"save model:epoch {}\".format(epoch))\n",
    "    if avg_train_loss < 0.001  and avg_val_loss.item()< 0.001 :\n",
    "        torch.save(model.state_dict(), str('B_')+save_path[:-4]+str(epoch)+'.pth')\n",
    "        print(\"save model:epoch {}\".format(epoch))\n",
    "# save last1\n",
    "torch.save(model.state_dict(), save_path[:-4]+'last.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
